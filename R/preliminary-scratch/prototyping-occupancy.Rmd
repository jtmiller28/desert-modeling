---
title: "prototyping-occupancy"
author: "JT Miller"
date: "2023-08-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A prototype of Occupancy Modeling: Checking to see if Vaughn Shirey's Occupancy Modeling methods can be used for the Sonoran Desert assemblages. The script: (data_process.R)[https://github.com/lmguzman/occ_historical/blob/main/analysis/odonates/scripts/data_process.R] by Vaughn was referenced to build this 

### Start out small: Model 1 common annual plant species: Rafinesquia neomexicana. 

Load Necessary Libraries 
```{r message=FALSE}
library(tidyverse); library(taxotools); library(data.table)
library(sf); library(cowplot); library(maps); library(mapdata)
library(ggpubr); library(nimble); library(raster); library(lubridate)
library(reclin); library(sp); library(rgdal); library(geosphere); library(plotKML)
```

Outline for Occupancy Modeling with Opportunistic Occurrence Data (as noted in Shirley et al. 2023)
      1. Determine the proportion of "Community Sampling Events" (CSEs) for the given species *Rafinesquia neomexicana*, where CSEs are defined as sampling events where more than 1 species is collected on the same day within 1km of eachother. 50% is the go-no-go cut off. 
      2. Decide on the Spatial and temporal scale of the analysis, as well as the size of the occupancy intervals & visit intervals
      3. Check the Range Overlap of the species
      4. Check taxonomic scale and Sampling methods. Should non-detections be inferred across the family and genus level? 
      5. Check whether visit history increases, decreases, or remains constant through occupancy intervals. 
      6. Remove sites that are only present for a _single_ occupancy interval. They will skew the visit history
      7. If visit history decreases through time and the probability of community sampling events is too low, re-eval from step 2 on. 
      

This prototyping is going to focus on modeling a one species as full modeling will be resource intensive, this analysis will focus on the Sonoran desert as the delimitation of Area. 

Step 1A: 
Bring in the shapefile of the Sonoran and all Plant occurrence data related to the Sonoran. 
```{r}
na_ecoregions_3 <- sf::read_sf("/blue/soltis/millerjared/pollen-project/Pollen-DB/raw-data/Ecoregions/na-level3-ecoregions/NA_CEC_Eco_Level3.shp") # EPA North American Ecoregion shapefiles

sonoran_shp <- subset(na_ecoregions_3, na_ecoregions_3$NA_L3NAME == "Sonoran Desert") # Subset for the shapefiles that include the Sonoran Desert

directory <- "/blue/soltis/millerjared/pollen-project/Pollen-DB/pollen-project-cluster/plant-gator-pulls-v0.4/specimen-info-datasets/" # Create a variable with the path of the dir
file_list <- list.files(directory, full.names = TRUE) # Create a list of all the file-names (which are the accepted species names in the dir)
species_tables <- list() # Create a species tables holding list
combined_table <- NULL # Initialize combined_table value as NULL 
check_line_count <- list() # Create a vector of line counts to check against parent data. 
for (i in 1:length(file_list)) { 
  df <- read.delim(file_list[[i]], fill = TRUE, quote = "") # Issue with fread dropping data, conversion necessary
  dt <- as.data.table(df) # Convert
  check_line_count[[i]] <- nrow(dt) # Check in place
  if ("Sonoran Desert" %in% dt$ecoregion) { # Require that Sonoran Desert be present for at least one of the species in the region. 
    dt[, (colnames(dt)) := lapply(.SD, as.character), .SDcols = colnames(dt)] 
    if (is.null(combined_table)) {
      combined_table <- dt
    } else {
      combined_table <- rbindlist(list(combined_table, dt), use.names = TRUE, fill = TRUE)
    }
  }
  print(paste0("table ", i, paste0(" of ", length(file_list)))) # Just a check so I can keep track of where in the loop we are
}

```
This is a quick check on read-continuity after noticing some odd behavior with fread. Feel free to SKIP as its just data checks. Current Status: Issues, but minimal (1-2 reads seem to be missing for 79 species) chat with Rob about SQL stuff. 
```{r}
# Check line count from bash-script reader to insure all lines were read 
count_read <- read.delim("/blue/soltis/millerjared/desert-modeling/outputs/line_counts.txt", header = FALSE) # see bash script for details.
run_counts <- combined_table[, .(num_rows = .N), by = accepted_name] # 
## Extract n_of_occ and accepted_name from the count_read bash file
count_read <- as.data.table(count_read)
count_read <- count_read[, n_of_occ := as.integer(gsub("^(\\d+).*", "\\1", V1)) - 1] # Also...headers count as a line so n_of_occ - 1
count_read <- count_read[, accepted_name := gsub(".*/([^/]+)\\.txt$", "\\1", V1)]
count_read <- count_read[, accepted_name := gsub("-", " ", accepted_name)]

common_names <- intersect(count_read$accepted_name, run_counts$accepted_name)

count_read <- count_read[accepted_name %in% common_names]
run_counts <- run_counts[accepted_name %in% common_names]

nrow(count_read) == nrow(run_counts) # Check to see if there is the same number of rows in each comparison table

## Check continuity 
merge_tb <- merge(count_read, run_counts, by = "accepted_name", suffixes = c("_1", "_2")) # Merge, add suffixes to avoid dup error

merge_tb <- merge_tb[, is_equivalent := n_of_occ == num_rows] # test equivalency
all_equivalent <- all(merge_tb$is_equivalent)
merge_tb[, .(.N), by = is_equivalent]
# So, in short...there are problems by 1-2 records. Unsure what is tripping this atm, this is prob a chat with Rob. For now im going with 'good enough' but will change things when we set up the SQL server. 
```

Step 1B: Buffer the Sonoran Shp file to account for community sampling that could occur on the region of interest's edges. Filter occurrence records to only include those found within the buffered shp.  
```{r}
sonoran_shp_b <- st_buffer(sonoran_shp, dist = 50000) # 50,000 m = 50km

combined_table_f <- combined_table[, latitude := as.numeric(latitude)] # Fix field class
combined_table_f <- combined_table_f[, longitude := as.numeric(longitude)]
combined_table_f <- combined_table[!is.na(latitude)] # Remove non-spatial records
combined_table_f <- combined_table_f[!is.na(longitude)]

# Convert occurrence data to spatial, transfrom to the shapefiles CRS
combined_table_f <- as.data.frame(combined_table_f) # restate as a data.frame for spatial conversion
occ_spat <- st_as_sf(combined_table_f, # convert the occurrence data to a spatial object
                     coords=c("longitude", "latitude"),
                     crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs",
                     remove = FALSE)  

occ_spat <- occ_spat %>% 
  st_transform(crs = st_crs(sonoran_shp_b)) # Transform CRS

# Filter the data according to the shapefile boundary 
occ_spat_f <- occ_spat[sonoran_shp_b,]
```

1C: Determine Community Sampling events for *Rafinesquia neomexicana*
```{r}
# Modify data to be non-spatial for faster processing
occ_f <- occ_spat_f %>% 
  st_drop_geometry()

# Some data prep
comm_sampling <- occ_f[, c("accepted_name", "eventDate", "latitude", "longitude")] # Select columns of interest
comm_sampling <- combined_table[coordinateUncertaintyInMeters < 100*1000] # must be less than 10,000m = 10km 
comm_sampling <- comm_sampling[!is.na(eventDate)] # Drop any NA eventDates
comm_sampling <- comm_sampling[, eventDate := gsub("T00:00:00", "", eventDate)] # Fix timestamped dates
comm_sampling <- comm_sampling[, clean_date := ymd(eventDate)] # put in standard format
comm_sampling <- comm_sampling[!is.na(clean_date)]
comm_sampling <- distinct(comm_sampling, accepted_name,clean_date, latitude, longitude, .keep_all = TRUE)
comm_sampling <- comm_sampling[clean_date > ymd("1950-01-01")] # Some reasonable baseline
comm_sampling <- comm_sampling[clean_date < ymd("2020-12-31")] # End Date (as this is probably about where we have climate data to)
comm_sampling <- comm_sampling[!is.na(latitude)]
comm_sampling <- comm_sampling[!is.na(longitude)]
n_obs_day <- table(comm_sampling$clean_date) # Create a table of cleaned dates. 
single_obs_dates <- names(n_obs_day[n_obs_day == 1]) # Single Obs Date
unique_dates <- names(n_obs_day[n_obs_day > 1])

# Create a for loop to look at each interval
cluster_lists <- list()
single_obs_data <- comm_sampling[clean_date %in% ymd(single_obs_dates)]
single_obs_data[, "cluster" := paste0(clean_date, "-", 1)]
cluster_lists[[1]] <- single_obs_data
counter <- 2
for(date_use in unique_dates){
  
  cur_date <- comm_sampling[clean_date == date_use]
  if("Rafinesquia neomexicana" %in% cur_date$accepted_name){ # Here, we grab it if it includes the species of interest
  lat_lon <- cur_date[, .(longitude, latitude)]
  
  xy <- SpatialPointsDataFrame(
    lat_lon, data.frame(ID = seq(1:nrow(lat_lon))),
    proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")) 
  
  
  mdist <- distm(xy) # Creates a geodesic distance matrix in meters
  hc <- hclust(as.dist(mdist), method = "complete") # Cluster all points using hierarchical approach
  
  d = 1000 # define a distance threshold (1km)
  
 
  
  cur_date[, "cluster" := paste0(clean_date, "-", cutree(hc, h=d))]
  
  cluster_lists[[counter]] <- cur_date
  
  counter <- counter + 1 
  
  print(paste0("subset ", date_use, paste0(" of ", max(unique_dates)))) 
  }
 
}
all_clusters <- rbindlist(cluster_lists)

size_clusters <- table(all_clusters$cluster) %>% table()

# percentage of community samplings
com_sampling_perc <- size_clusters[1]/sum(size_clusters[1:length(size_clusters)])

print(com_sampling_perc) # Clears the 50% requirement. 
```

2. Decide on the Spatial and temporal scale of the analysis, as well as the size of the occupancy intervals & visit intervals
```{r}
# First lets take a look at the spatial representation
## set the project crs to North American Equal Area Conic (Albers)
crs_1 <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m"
## Change our buffered shapefile to match this equal area proj
sonoran_shp_b <- st_transform(sonoran_shp_b, st_crs(crs_1))
## Make a Grid with 50km gridcells
grid_1 <-st_make_grid(extent(sonoran_shp_b), cellsize = 50*1000, square = FALSE) %>% # Make a hexagonal grid from the extents of our buffered region
          st_as_sf(crs = crs_1) %>% # Assign CRS
          dplyr::mutate(GID = row_number()) # Track Grid IDs by row number in our datatable


touching_cells <- grid_1[sonoran_shp_b,]

ggplot() + 
  geom_sf(sonoran_shp_b, mapping = aes(), fill = "yellow")+
  geom_sf(sonoran_shp, mapping = aes(), fill = "darkorange") +
  geom_sf(touching_cells, mapping = aes(), alpha = 0.2) +
  theme_bw() +
  ggtitle("Hexagon Intersection Method of Creating a grid over the Sonoran")
  

## Make convex-hull to show known range
occ_f <- st_as_sf(occ_f, 
           coords=c("longitude", "latitude"),
           crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs") 

occ_f <- occ_f %>% st_transform(crs_1) # transform to equal area proj

# make range maps from convex polygons
range <- list()
sp_list <- unique(occ_f$accepted_name)
for(i in 1:length(sp_list)){
  range[[i]] <- st_convex_hull(st_union(filter(occ_f,
                                               accepted_name==sp_list[i])))
}

ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(grid_1, mapping=aes(), fill=NA, color="grey")+
  geom_sf(range[[1]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[1]), mapping=aes(), color="cyan")+
  geom_sf(range[[2]], mapping=aes(), fill="red", color="red", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[2]), mapping=aes(), color="red")+
  geom_sf(range[[10]], mapping=aes(), fill="gold", color="gold", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[10]), mapping=aes(), color="gold")+
  theme_map()

ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(grid_1, mapping=aes(), fill=NA, color="grey")+
  geom_sf(range[[3872]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[3872]), mapping=aes(), color="royalblue")+
  ggtitle(paste0("Species Range for ", sp_list[3872])) +
  theme_map()

# Vaughn's code uses a matrix to assess range overlap, but this a bit challenging with irregularly shaped grids. By their suggestion lets try vectorizing: Using the hex's unique GIDs we're going to index the overlap in ranges. 

## Proof of concept: 
grid_copy <- touching_cells  # Make a copy of the hex cells that matter
test <- st_intersects(touching_cells, range[[3872]]) # Find all of the intersecting hex cells that touch the range of R. neomexicana

grid_copy$intersection <- as.integer(test) # Returns 1s for the TRUE values, for some reason returns NAs instead of 0s for FALSE? 

grid_copy <- grid_copy %>% 
  mutate(intersection = case_when( # Use a case_when to ammend this
    !is.na(intersection) ~ 1, 
    TRUE ~ 0
  ))

# Show a visual... 
ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(touching_cells, mapping = aes(), color = "grey", alpha = 0.2) +
  geom_sf(grid_copy[grid_copy$intersection == 1,], mapping=aes(), fill=NA, color="red")+
  geom_sf(range[[3872]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[3872]), mapping=aes(), color="royalblue")+
  ggtitle(paste0("Species Range for ", sp_list[3872])) +
  theme_map()
```
Now Create a loop for indexing all of the present cells per plant range. 
```{r}

```


Extraneous code Im keeping around but is not currently part of the Analysis
```{r}
# For determining species range overlaps: The Matrix Gridding Method (Unfinished)
## Intersect Range Map with grid
grid_extent <- extent(touching_cells)
xmin <- grid_extent[1]
xmax <- grid_extent[2]
ymin <- grid_extent[3]
ymax <- grid_extent[4]
cellsize <- 50*1000
xr <- (xmax - xmin)/cellsize # Calc using a xmax-xmin/cell_size?
yr <- (ymax - ymin)/cellsize
grid_int <- matrix(as.matrix(st_intersects(touching_cells, range)), nrow = xr, ncol = y)

```



      