---
title: "prototyping-occupancy"
author: "JT Miller"
date: "2023-08-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A prototype of Occupancy Modeling: Checking to see if Vaughn Shirey's Occupancy Modeling methods can be used for the Sonoran Desert assemblages. The script: (data_process.R)[https://github.com/lmguzman/occ_historical/blob/main/analysis/odonates/scripts/data_process.R] by Vaughn was referenced to build this 

### Start out small: Model 1 common annual plant species: Rafinesquia neomexicana. 

Load Necessary Libraries 
```{r message=FALSE}
library(tidyverse); library(taxotools); library(data.table)
library(sf); library(cowplot); library(maps); library(mapdata)
library(ggpubr); library(nimble); library(raster); library(lubridate)
library(reclin); library(sp); library(rgdal); library(geosphere); library(plotKML)
```

Outline for Occupancy Modeling with Opportunistic Occurrence Data (as noted in Shirley et al. 2023)
      1. Determine the proportion of "Community Sampling Events" (CSEs) for the given species *Rafinesquia neomexicana*, where CSEs are defined as sampling events where more than 1 species is collected on the same day within 1km of eachother. 50% is the go-no-go cut off. 
      2. Decide on the Spatial and temporal scale of the analysis, as well as the size of the occupancy intervals & visit intervals
      3. Check the Range Overlap of the species
      4. Check taxonomic scale and Sampling methods. Should non-detections be inferred across the family and genus level? 
      5. Check whether visit history increases, decreases, or remains constant through occupancy intervals. 
      6. Remove sites that are only present for a _single_ occupancy interval. They will skew the visit history
      7. If visit history decreases through time and the probability of community sampling events is too low, re-eval from step 2 on. 
      

This prototyping is going to focus on modeling a one species as full modeling will be resource intensive, this analysis will focus on the Sonoran desert as the delimitation of Area. 

Step 1A: 
Bring in the shapefile of the Sonoran and all Plant occurrence data related to the Sonoran. 
```{r eval=FALSE, include=FALSE}
na_ecoregions_3 <- sf::read_sf("/blue/soltis/millerjared/pollen-project/Pollen-DB/raw-data/Ecoregions/na-level3-ecoregions/NA_CEC_Eco_Level3.shp") # EPA North American Ecoregion shapefiles

sonoran_shp <- subset(na_ecoregions_3, na_ecoregions_3$NA_L3NAME == "Sonoran Desert") # Subset for the shapefiles that include the Sonoran Desert

directory <- "/blue/soltis/millerjared/pollen-project/Pollen-DB/pollen-project-cluster/plant-gator-pulls-v0.4/specimen-info-datasets/" # Create a variable with the path of the dir
file_list <- list.files(directory, full.names = TRUE) # Create a list of all the file-names (which are the accepted species names in the dir)
species_tables <- list() # Create a species tables holding list
combined_table <- NULL # Initialize combined_table value as NULL 
check_line_count <- list() # Create a vector of line counts to check against parent data. 
for (i in 1:length(file_list)) { 
  df <- read.delim(file_list[[i]], fill = TRUE, quote = "") # Issue with fread dropping data, conversion necessary
  dt <- as.data.table(df) # Convert
  check_line_count[[i]] <- nrow(dt) # Check in place
  if ("Sonoran Desert" %in% dt$ecoregion) { # Require that Sonoran Desert be present for at least one of the species in the region. 
    dt[, (colnames(dt)) := lapply(.SD, as.character), .SDcols = colnames(dt)] 
    if (is.null(combined_table)) {
      combined_table <- dt
    } else {
      combined_table <- rbindlist(list(combined_table, dt), use.names = TRUE, fill = TRUE)
    }
  }
  print(paste0("table ", i, paste0(" of ", length(file_list)))) # Just a check so I can keep track of where in the loop we are
}

```

Write-out as a csv so we dont need to run this time-consuming codeblock above for future runs
```{r eval=FALSE, include=FALSE}
write.csv(combined_table, "/blue/soltis/millerjared/desert-modeling/data/sonoran-extent-plants.csv", row.names = FALSE)
```



```{r}
combined_table <- fread("/blue/soltis/millerjared/desert-modeling/data/sonoran-extent-plants.csv")
```



This is a quick check on read-continuity after noticing some odd behavior with fread. Feel free to SKIP as its just data checks. Current Status: Issues, but minimal (1-2 reads seem to be missing for 79 species) chat with Rob about SQL stuff. 
```{r eval=FALSE, include=FALSE}
# Check line count from bash-script reader to insure all lines were read 
count_read <- read.delim("/blue/soltis/millerjared/desert-modeling/outputs/line_counts.txt", header = FALSE) # see bash script for details.
run_counts <- combined_table[, .(num_rows = .N), by = accepted_name] # 
## Extract n_of_occ and accepted_name from the count_read bash file
count_read <- as.data.table(count_read)
count_read <- count_read[, n_of_occ := as.integer(gsub("^(\\d+).*", "\\1", V1)) - 1] # Also...headers count as a line so n_of_occ - 1
count_read <- count_read[, accepted_name := gsub(".*/([^/]+)\\.txt$", "\\1", V1)]
count_read <- count_read[, accepted_name := gsub("-", " ", accepted_name)]

common_names <- intersect(count_read$accepted_name, run_counts$accepted_name)

count_read <- count_read[accepted_name %in% common_names]
run_counts <- run_counts[accepted_name %in% common_names]

nrow(count_read) == nrow(run_counts) # Check to see if there is the same number of rows in each comparison table

## Check continuity 
merge_tb <- merge(count_read, run_counts, by = "accepted_name", suffixes = c("_1", "_2")) # Merge, add suffixes to avoid dup error

merge_tb <- merge_tb[, is_equivalent := n_of_occ == num_rows] # test equivalency
all_equivalent <- all(merge_tb$is_equivalent)
merge_tb[, .(.N), by = is_equivalent]
# So, in short...there are problems by 1-2 records. Unsure what is tripping this atm, this is prob a chat with Rob. For now im going with 'good enough' but will change things when we set up the SQL server. 
```

Step 1B.1: Buffer the Sonoran Shp file to account for community sampling that could occur on the region of interest's edges. Filter occurrence records to only include those found within the buffered shp.  
```{r}
na_ecoregions_3 <- sf::read_sf("/blue/soltis/millerjared/pollen-project/Pollen-DB/raw-data/Ecoregions/na-level3-ecoregions/NA_CEC_Eco_Level3.shp") # EPA North American Ecoregion shapefiles

sonoran_shp <- subset(na_ecoregions_3, na_ecoregions_3$NA_L3NAME == "Sonoran Desert") # Subset for the shapefiles that include the Sonoran Desert

sonoran_shp_b <- st_buffer(sonoran_shp, dist = 50000) # 50,000 m = 50km

combined_table_f <- combined_table[, latitude := as.numeric(latitude)] # Fix field class
combined_table_f <- combined_table_f[, longitude := as.numeric(longitude)]
combined_table_f <- combined_table[!is.na(latitude)] # Remove non-spatial records
combined_table_f <- combined_table_f[!is.na(longitude)]

# Convert occurrence data to spatial, transfrom to the shapefiles CRS
combined_table_f <- as.data.frame(combined_table_f) # restate as a data.frame for spatial conversion
occ_spat <- st_as_sf(combined_table_f, # convert the occurrence data to a spatial object
                     coords=c("longitude", "latitude"),
                     crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs",
                     remove = FALSE)  

occ_spat <- occ_spat %>% 
  st_transform(crs = st_crs(sonoran_shp_b)) # Transform CRS

# Filter the data according to the shapefile boundary 
occ_spat_f <- occ_spat[sonoran_shp_b,]

# Now transform data back to wgs84 projection for further calculations
occ_spat_f <- st_transform(occ_spat_f, crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
```
1B.2: Use number of samples to build an adequate species list in terms of sampling
```{r}
sp_list <- as.data.table(table(occ_spat_f$accepted_name)) %>%
  rename(accepted_name = V1) %>% 
  dplyr::filter(N >= 100) # Must have more than 100 entries to be considered. Arbitrary? 
  
```

1C: Determine Community Sampling events for *Rafinesquia neomexicana*
```{r}
# Modify data to be non-spatial for faster processing
occ_f <- occ_spat_f %>% 
  st_drop_geometry()



# Some data prep
comm_sampling <- occ_f[, c("accepted_name", "eventDate", "latitude", "longitude", "recordedBy")] # Select columns of interest
comm_sampling <- combined_table[coordinateUncertaintyInMeters < 100*1000 | is.na(coordinateUncertaintyInMeters)] # must be less than 10,000m = 10km 
comm_sampling <- comm_sampling[!is.na(eventDate)] # Drop any NA eventDates
comm_sampling <- comm_sampling[, eventDate := gsub("T00:00:00", "", eventDate)] # Fix timestamped dates
comm_sampling <- comm_sampling[, clean_date := ymd(eventDate)] # put in standard format
comm_sampling <- comm_sampling[!is.na(clean_date)]
comm_sampling <- distinct(comm_sampling, accepted_name, clean_date, latitude, longitude, recordedBy, .keep_all = TRUE)
comm_sampling <- comm_sampling[clean_date > ymd("1950-01-01")] # Some reasonable baseline
comm_sampling <- comm_sampling[clean_date < ymd("2020-12-31")] # End Date (as this is probably about where we have climate data to)
comm_sampling <- comm_sampling[!is.na(latitude)]
comm_sampling <- comm_sampling[!is.na(longitude)]
n_obs_day <- table(comm_sampling$clean_date) # Create a table of cleaned dates. 
single_obs_dates <- names(n_obs_day[n_obs_day == 1]) # Single Obs Date
unique_dates <- names(n_obs_day[n_obs_day > 1])

# Create a for loop to look at each interval
cluster_lists <- list()
single_obs_data <- comm_sampling[clean_date %in% ymd(single_obs_dates)]
single_obs_data[, "cluster" := paste0(clean_date, "-", 1)]
cluster_lists[[1]] <- single_obs_data
counter <- 2
for(date_use in unique_dates){
  
  cur_date <- comm_sampling[clean_date == date_use]
  #if("Rafinesquia neomexicana" %in% cur_date$accepted_name){ # Here, we grab it if it includes the species of interest
  lat_lon <- cur_date[, .(longitude, latitude)]
  
  xy <- SpatialPointsDataFrame(
    lat_lon, data.frame(ID = seq(1:nrow(lat_lon))),
    proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")) 
  
  
  mdist <- distm(xy) # Creates a geodesic distance matrix in meters
  hc <- hclust(as.dist(mdist), method = "complete") # Cluster all points using hierarchical approach
  
  d = 1000 # define a distance threshold (1km)
  
 
  
  cur_date[, "cluster" := paste0(clean_date, "-", cutree(hc, h=d))]
  
  cluster_lists[[counter]] <- cur_date
  
  counter <- counter + 1 
  
  print(paste0("subset ", date_use, paste0(" of ", max(unique_dates)))) 
  #}
 
}
all_clusters <- rbindlist(cluster_lists)

size_clusters <- table(all_clusters$cluster) %>% table()

# percentage of community samplings
com_sampling_perc <- size_clusters[1]/sum(size_clusters[1:length(size_clusters)])

print(com_sampling_perc) # Clears the 50% requirement. 
```

1.C Edited. (Removing as much tidyverse as possible to speed things up)
```{r}
# Modify data to be non-spatial for faster processing
occ_f <- occ_spat_f %>% 
  st_drop_geometry()

# Change to data.table format
occ_f <- as.data.table(occ_f)

# Grab only records that have harmonized names that match sp_list
occ_dat <- occ_f[accepted_name %in% sp_list$Var1]


# Remove records that contain too much uncertainty >100km of noted error 
occ_dat <- occ_dat[coordinateUncertaintyInMeters < 100*1000 | is.na(coordinateUncertaintyInMeters)]

# Fix dates: Some data is malformed for date standards
occ_dat <- occ_dat[!is.na(eventDate)] # Drop any NA eventDates
occ_dat <- occ_dat[, fixedEventDate := gsub("T\\d+:\\d+:\\d+", "", eventDate)] # Fixes timestamped dates
occ_dat <- occ_dat[, date_clean := ymd(fixedEventDate)]
occ_dat <- occ_dat[!is.na(date_clean)]

# Only grab dates of interest: 1950 - 2020
occ_dat <- occ_dat[date_clean > ymd("1950-01-01") & date_clean <= ymd("2020-12-31")] # create a bounds
# any(is.na(unique(occ_dat$eventDate))) == TRUE 

# Grab selected fields for analysis
occ_dat <- occ_dat[, c("accepted_name", "date_clean", "recordedBy", "latitude", "longitude", "year")]

# Approximate the number of 'community sampling events' that are relevant to our species of focus 'Rafinesquia neomexicana' 
occ_dat <- distinct(occ_dat, accepted_name, date_clean, latitude, longitude, .keep_all = TRUE) # insure distinct
occ_dat <- occ_dat[!is.na(latitude)] # remove records without latitude 
occ_dat <- occ_dat[!is.na(longitude)] # remove records without longitude
occ_dat <- occ_dat[!is.na(recordedBy)] # remove records without recordedBy 

comm_coll <- occ_dat # dt for our community sampling 
comm_coll$recordedBy %>% table() %>% sort(decreasing = TRUE) %>% head() # Display some collector metrics 

# Cluster the observations by same day within 1km square areas as our proxy for 'community sampling events'
n_obs_day <- table(comm_coll$date_clean)
single_obs_dates <- names(n_obs_day[n_obs_day == 1])
unique_dates <- names(n_obs_day[n_obs_day > 1])

cluster_lists <- list()
sin_obs_data <- comm_coll[date_clean %in% ymd(single_obs_dates)]
sin_obs_data[, "cluster" := paste0(date_clean, "-", 1)]
cluster_lists[[1]] <- sin_obs_data
counter <- 2

for(date_use in unique_dates){
  
  cur_date <- comm_coll[date_clean == date_use]
    if("Rafinesquia neomexicana" %in% cur_date$accepted_name){ # Here, we grab it if it includes the species of interest
  lat_lon <- cur_date[,.(longitude, latitude)]
  
  xy <- SpatialPointsDataFrame(
    lat_lon, data.frame(ID=seq(1:nrow(lat_lon))),
    proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84"))
  
  # use the distm function to generate a geodesic distance matrix in meters
  mdist <- distm(xy)
  
  # cluster all points using a hierarchical clustering approach
  hc <- hclust(as.dist(mdist), method="complete")
  
  # define the distance threshold
  d=1000
  
  cur_date[, "cluster" := paste0(date_clean, "-", cutree(hc, h=d))]
  
  cluster_lists[[counter]] <- cur_date
  
  counter <- counter+1
  print(paste0("subset ", date_use, paste0(" of ", max(unique_dates)))) # Print out dates to note where we are 
} # End of if statement 
} # End of for loop
all_clusters <- rbindlist(cluster_lists)

size_clusters <- table(all_clusters$cluster) %>% table()

# percentage of community samplings
print(size_clusters[1]/sum(size_clusters[1:length(size_clusters)]))

```

09-26 v. 
```{r}
occ_dat <- occ_spat_f %>% 
  dplyr::filter(coordinateUncertaintyInMeters < 100*1000 | is.na(coordinateUncertaintyInMeters)) # remove data based upon uncertainty 

versioned_dat <- occ_dat

sp_list <- as.data.frame(table(occ_dat$accepted_name)) %>% 
  dplyr::filter(Freq >= 100)

# occ_dat <- occ_dat %>%  # Fix up the dates which have those problematic time stamps
#   mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>% 
#   mutate(date_clean = ymd(fixedEventDate))

comm_collxs <- versioned_dat %>% # take from versioned dat...
  dplyr::select(accepted_name, eventDate, recordedBy, latitude, longitude) %>%  # select cols of interest
  dplyr::distinct() %>% # run distinct on cols of interest
  dplyr::filter(!is.na(eventDate), !is.na(latitude), !is.na(longitude)) %>%  # remove NAs for location and date
  dplyr::filter(!is.na(recordedBy)) %>% # remove NAs for no recordedBy
  mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>%  # fix time stamped dates
  mutate(date_clean = ymd(fixedEventDate)) %>% # fix up date into ymd using lubridate
  filter(date_clean > ymd("1950-01-01")) %>% # set a min date
  filter(date_clean >= ymd("2020-12-31")) # set a max date
  
comm_collxs$recordedBy %>% table() %>% # take the comms_collxs and create a table
  sort(decreasing = TRUE) %>% # Sort by greatest to least
  head() # take the first 5 

# Look at community sampling by same day within 1km of eachother...
comm_collxs <- versioned_dat %>%  # take from versioned dat...
  dplyr::select(accepted_name, eventDate, recordedBy, latitude, longitude) %>% 
  distinct() %>% 
  dplyr::filter(!is.na(eventDate), !is.na(latitude), !is.na(longitude)) %>% 
  dplyr::filter(!is.na(recordedBy)) %>% 
  mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>%  # fix time stamped dates
  mutate(date_clean = ymd(fixedEventDate)) %>% # fix up date into ymd using lubridate
  filter(date_clean > ymd("1950-01-01")) %>% # set a min date
  filter(date_clean <= ymd("2020-12-31")) %>% 
  data.table()

n_obs_day <- table(comm_collxs$date_clean) # create a freq table of the dates in our comm_collxs
single_obs_dates <- names(n_obs_day[n_obs_day == 1]) # where only single observations were made
unique_dates <- names(n_obs_day[n_obs_day > 1]) # where multiple observations were made

cluster_lists <- list() # create a holder for the cluster_lists
sin_obs_data <- comm_collxs[date_clean %in% ymd(single_obs_dates)] # create a dt that contains the data that corresponds with the single obs dates
sin_obs_data[, "cluster" := paste0(date_clean, "-", 1)] # create a new field called cluster, paste the clean date from the single obs dates + "-1" to denote that these are single obs events
cluster_lists[[1]] <- sin_obs_data # embed these as the first element in cluster lists
counter <- 2 # establish a counter var

for(date_use in unique_dates){ # recall that date_use is i here... pulling from our unique_dates (Mult observs)
  cur_date <- comm_collxs[date_clean == date_use] # subset comm_collxs by unique mult dates
  
  lat_lon <- cur_date[, .(longitude, latitude)] # subset for only lat lon
  
  xy <- SpatialPointsDataFrame( # create a spatial points dataframe 
    lat_lon, # specified in lat lon
    data.frame(ID = seq(1:nrow(lat_lon))),  # create a field called ID based upon each lat lon
    proj4string = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84") # specify our crs as wgs84
    )
  
  # use distm to generate a geodesic distance matrix in meters
  mdist <- distm(xy)
  
  # cluster all points using a hierarchical clustering approach
  hc <- hclust(as.dist(mdist), method = "complete")
  
  # define the distance threshold to use
  d = 1000 # 1km
  
  cur_date[, "cluster" := paste0(date_clean, "-", cutree(hc, h=d))] # create the cluster field based upon the number of clustered points
  
  cluster_lists[[counter]] <- cur_date # storage for cluster_lists based upon where we are 
  
  counter <- counter + 1 # add one per iteration
  
  print(paste0("subset ", date_use, paste0(" of ", max(unique_dates)))) # Print out dates to note where we are 
}

all_clusters <- rbindlists(cluster_lists) # rbind our cluster lists into a df

size_clusters <- table(all_clusters$cluster) %>% # create a freq table
  table() # duplicates(?)

size_clusters[1]/sum(size_clusters[1:length(size_clusters)]) # measure the proportion of community samplings to total samplings

```
v09-26 without recordedBy accounted
```{r}
occ_dat <- occ_spat_f %>% 
  dplyr::filter(coordinateUncertaintyInMeters < 100*1000 | is.na(coordinateUncertaintyInMeters)) # remove data based upon uncertainty 

versioned_dat <- occ_dat

sp_list <- as.data.frame(table(occ_dat$accepted_name)) %>% 
  dplyr::filter(Freq >= 100)

# occ_dat <- occ_dat %>%  # Fix up the dates which have those problematic time stamps
#   mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>% 
#   mutate(date_clean = ymd(fixedEventDate))

comm_collxs <- versioned_dat %>% # take from versioned dat...
  dplyr::select(accepted_name, eventDate, recordedBy, latitude, longitude) %>%  # select cols of interest
  dplyr::distinct() %>% # run distinct on cols of interest
  dplyr::filter(!is.na(eventDate), !is.na(latitude), !is.na(longitude)) %>%  # remove NAs for location and date
  dplyr::filter(!is.na(recordedBy)) %>% # remove NAs for no recordedBy
  mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>%  # fix time stamped dates
  mutate(date_clean = ymd(fixedEventDate)) %>% # fix up date into ymd using lubridate
  filter(date_clean > ymd("1950-01-01")) %>% # set a min date
  filter(date_clean >= ymd("2020-12-31")) # set a max date
  
comm_collxs$recordedBy %>% table() %>% # take the comms_collxs and create a table
  sort(decreasing = TRUE) %>% # Sort by greatest to least
  head() # take the first 5 

# Look at community sampling by same day within 1km of eachother...
comm_collxs <- versioned_dat %>%  # take from versioned dat...
  dplyr::select(accepted_name, eventDate, latitude, longitude) %>% 
  distinct() %>% 
  dplyr::filter(!is.na(eventDate), !is.na(latitude), !is.na(longitude)) %>% 
  dplyr::filter(!is.na(recordedBy)) %>% 
  mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>%  # fix time stamped dates
  mutate(date_clean = ymd(fixedEventDate)) %>% # fix up date into ymd using lubridate
  filter(date_clean > ymd("1950-01-01")) %>% # set a min date
  filter(date_clean <= ymd("2020-12-31")) %>% 
  data.table()

n_obs_day <- table(comm_collxs$date_clean) # create a freq table of the dates in our comm_collxs
single_obs_dates <- names(n_obs_day[n_obs_day == 1]) # where only single observations were made
unique_dates <- names(n_obs_day[n_obs_day > 1]) # where multiple observations were made

cluster_lists <- list() # create a holder for the cluster_lists
sin_obs_data <- comm_collxs[date_clean %in% ymd(single_obs_dates)] # create a dt that contains the data that corresponds with the single obs dates
sin_obs_data[, "cluster" := paste0(date_clean, "-", 1)] # create a new field called cluster, paste the clean date from the single obs dates + "-1" to denote that these are single obs events
cluster_lists[[1]] <- sin_obs_data # embed these as the first element in cluster lists
counter <- 2 # establish a counter var

for(date_use in unique_dates){ # recall that date_use is i here... pulling from our unique_dates (Mult observs)
  cur_date <- comm_collxs[date_clean == date_use] # subset comm_collxs by unique mult dates
  
  lat_lon <- cur_date[, .(longitude, latitude)] # subset for only lat lon
  
  xy <- SpatialPointsDataFrame( # create a spatial points dataframe 
    lat_lon, # specified in lat lon
    data.frame(ID = seq(1:nrow(lat_lon))),  # create a field called ID based upon each lat lon
    proj4string = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84") # specify our crs as wgs84
    )
  
  # use distm to generate a geodesic distance matrix in meters
  mdist <- distm(xy)
  
  # cluster all points using a hierarchical clustering approach
  hc <- hclust(as.dist(mdist), method = "complete")
  
  # define the distance threshold to use
  d = 1000 # 1km
  
  cur_date[, "cluster" := paste0(date_clean, "-", cutree(hc, h=d))] # create the cluster field based upon the number of clustered points
  
  cluster_lists[[counter]] <- cur_date # storage for cluster_lists based upon where we are 
  
  counter <- counter + 1 # add one per iteration
  
  print(paste0("subset ", date_use, paste0(" of ", max(unique_dates)))) # Print out dates to note where we are 
}

all_clusters <- rbindlists(cluster_lists) # rbind our cluster lists into a df

size_clusters <- table(all_clusters$cluster) %>% # create a freq table
  table() # duplicates(?)

size_clusters[1]/sum(size_clusters[1:length(size_clusters)]) # measure the proportion of community samplings to total samplings

```








Check to see if Vaughn's verbatim code gives the same result
```{r}
# read in GBIF and iDigBio data
gbif_dat <- occ_spat_f %>%
  dplyr::filter(coordinateUncertaintyInMeters < 100*1000 | is.na(coordinateUncertaintyInMeters))

occ_dat <- gbif_dat

# filter the occurrence data to a sensible time frame (1970s-2010s)
sp_list <- as.data.frame(table(occ_dat$accepted_name)) %>%
  dplyr::filter(Freq >= 100)

occ_dat <- occ_dat %>% # Removed: between(year, 1970, 2020),  from Vaughn's code
  filter(!is.na(accepted_name), accepted_name!="", accepted_name %in% sp_list$Var1) %>%
  dplyr::select(accepted_name, year, latitude, longitude)
head(occ_dat)
plot(occ_dat$longitude, occ_dat$latitude)


# approximate the number of "community visits" using event dates, collectors, and locations
comm_coll <- gbif_dat %>%
  dplyr::select(accepted_name, eventDate, latitude, longitude )%>% # removed the recordedBy after chatting with vaughn
  distinct() %>% 
  filter(!is.na(eventDate), !is.na(latitude), !is.na(longitude)) %>% 
  #filter(!is.na(recordedBy)) %>%
   mutate(fixedEventDate = gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>% 
  mutate(date_clean = ymd(fixedEventDate)) %>% 
  filter(date_clean > ymd("1970-01-01")) %>% 
  filter(date_clean <= ymd("2020-12-31")) %>%
  data.table()

comm_coll$recordedBy %>% table() %>% sort(decreasing = TRUE) %>% head()

n_obs_day <- table(comm_coll$date_clean)
single_obs_dates <- names(n_obs_day[n_obs_day == 1])
unique_dates <- names(n_obs_day[n_obs_day > 1])

cluster_lists <- list()
sin_obs_data <- comm_coll[date_clean %in% ymd(single_obs_dates)]
sin_obs_data[, "cluster" := paste0(date_clean, "-", 1)]
cluster_lists[[1]] <- sin_obs_data
counter <- 2

for(date_use in unique_dates){
  
  cur_date <- comm_coll[date_clean == date_use]
  #if("Rafinesquia neomexicana" %in% cur_date$accepted_name){ # Here, we grab it if it includes the species of interest
  lat_lon <- cur_date[,.(longitude, latitude)]
  
  xy <- SpatialPointsDataFrame(
    lat_lon, data.frame(ID=seq(1:nrow(lat_lon))),
    proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84"))
  
  # use the distm function to generate a geodesic distance matrix in meters
  mdist <- distm(xy)
  
  # cluster all points using a hierarchical clustering approach
  hc <- hclust(as.dist(mdist), method="complete")
  
  # define the distance threshold
  d=1000
  
  cur_date[, "cluster" := paste0(date_clean, "-", cutree(hc, h=d))]
  
  cluster_lists[[counter]] <- cur_date
  
  counter <- counter+1
  #}
}

all_clusters <- rbindlist(cluster_lists)

size_clusters <- table(all_clusters$cluster) %>% table()

# percentage of community samplings
size_clusters[1]/sum(size_clusters[1:length(size_clusters)])

```

1.D Tidy up the table for spatial/temporal scale analysis on the relevant species 
```{r}
occ_dat <- occ_f %>% 
  filter(!is.na(accepted_name), accepted_name!= "", accepted_name %in% sp_list$accepted_name) %>% # Filter records based on valid name + 100 record quota
  filter(coordinateUncertaintyInMeters < 100*1000 | is.na(coordinateUncertaintyInMeters)) %>%  # Filter records based on uncertainty
  filter(is.na(eventDate)) %>%  # Remove records with NA date
  mutate(eventDate = gsub("T00:00:00", "", eventDate)) %>%  # fix timestamped dates
  mutate(clean_date = ymd(eventDate)) %>%  # Standardize dates
  filter(!is.na(clean_date)) %>%  # Remove NA cleaned dates 
  filter(clean_date > ymd("1950-01-01") & clean_date < ymd("2020-12-31")) # Only take data for this timeframe
  

  
  
```




2. Decide on the Spatial and temporal scale of the analysis, as well as the size of the occupancy intervals & visit intervals
```{r}
# First lets take a look at the spatial representation
## set the project crs to North American Equal Area Conic (Albers)
crs_1 <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m"
## Change our buffered shapefile to match this equal area proj
sonoran_shp_b <- st_transform(sonoran_shp_b, st_crs(crs_1))
## Make a Grid with 50km gridcells
grid_1 <-st_make_grid(extent(sonoran_shp_b), cellsize = 50*1000, square = FALSE) %>% # Make a hexagonal grid from the extents of our buffered region
          st_as_sf(crs = crs_1) %>% # Assign CRS
          dplyr::mutate(GID = row_number()) # Track Grid IDs by row number in our datatable


touching_cells <- grid_1[sonoran_shp_b,]

ggplot() + 
  geom_sf(sonoran_shp_b, mapping = aes(), fill = "yellow")+
  geom_sf(sonoran_shp, mapping = aes(), fill = "darkorange") +
  geom_sf(touching_cells, mapping = aes(), alpha = 0.2) +
  theme_bw() +
  ggtitle("Hexagon Intersection Method of Creating a grid over the Sonoran")
  

## Make convex-hull to show known range
occ_f <- st_as_sf(occ_f, 
           coords=c("longitude", "latitude"),
           crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs") 

occ_f <- occ_f %>% st_transform(crs_1) # transform to equal area proj

# make range maps from convex polygons
range <- list()
sp_list <- unique(occ_f$accepted_name)
for(i in 1:length(sp_list)){
  range[[i]] <- st_convex_hull(st_union(filter(occ_f,
                                               accepted_name==sp_list[i])))
}

ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(grid_1, mapping=aes(), fill=NA, color="grey")+
  geom_sf(range[[1]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[1]), mapping=aes(), color="cyan")+
  geom_sf(range[[2]], mapping=aes(), fill="red", color="red", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[2]), mapping=aes(), color="red")+
  geom_sf(range[[10]], mapping=aes(), fill="gold", color="gold", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[10]), mapping=aes(), color="gold")+
  theme_map()

ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(grid_1, mapping=aes(), fill=NA, color="grey")+
  geom_sf(range[[3872]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[3872]), mapping=aes(), color="royalblue")+
  ggtitle(paste0("Species Range for ", sp_list[3872])) +
  theme_map()

# Vaughn's code uses a matrix to assess range overlap, but this a bit challenging with irregularly shaped grids. By their suggestion lets try vectorizing: Using the hex's unique GIDs we're going to index the overlap in ranges. 

## Proof of concept: 
grid_copy <- touching_cells  # Make a copy of the hex cells that matter
test <- st_intersects(touching_cells, range[[3872]]) # Find all of the intersecting hex cells that touch the range of R. neomexicana

grid_copy$intersection <- as.integer(test) # Returns 1s for the TRUE values, for some reason returns NAs instead of 0s for FALSE? 

grid_copy <- grid_copy %>% 
  mutate(intersection = case_when( # Use a case_when to ammend this
    !is.na(intersection) ~ 1, 
    TRUE ~ 0
  ))

# Show a visual... 
ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(touching_cells, mapping = aes(), color = "grey", alpha = 0.2) +
  geom_sf(grid_copy[grid_copy$intersection == 1,], mapping=aes(), fill=NA, color="red")+
  geom_sf(range[[3872]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[3872]), mapping=aes(), color="royalblue")+
  ggtitle(paste0("Species Range for ", sp_list[3872])) +
  theme_map()
```
Now Create a loop for indexing all of the present cells per plant range. 
```{r}
grid_int <- list()
for(i in 1:length(range)){
  grid_copy <- touching_cells # Create a fresh copy per run
  hex_int <- st_intersects(touching_cells, range[[i]]) # Return a logical output for whether the species range intersects with the hex cells
  grid_copy$intersection <- as.integer(hex_int) # Returns 1s for the TRUE values, for some reason returns NAs instead of 0s for FALSE (?) 
  grid_copy <- grid_copy %>% 
  mutate(intersection = case_when( # Use a case_when to ammend this
    !is.na(intersection) ~ 1, 
    TRUE ~ 0))
  
  grid_int[[i]] <- grid_copy # And store these intersection grids
  print(paste0("species ", i, paste0(" of ", length(range))))
}

# Check...
ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(touching_cells, mapping = aes(), color = "grey", alpha = 0.2) +
  geom_sf(grid_int[[3872]][grid_int[[3872]]$intersection == 1,], mapping=aes(), fill=NA, color="red")+
  geom_sf(range[[3872]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[3872]), mapping=aes(), color="royalblue")+
  ggtitle(paste0("Species Range for ", sp_list[3872])) +
  theme_map()

## assign occurrences to the indexes 
sp_list <- sp_list %>% as.data.frame()
colnames(sp_list) <- c("accepted_name")

sp_list <- sp_list %>%
  dplyr::mutate(SPID=row_number())


occ_f$year <- as.integer(occ_f$year)

occ_grid <- occ_f %>%
  st_intersection(touching_cells) %>% # Intersect with the toching grids
  dplyr::mutate(era=(year-year%%10)) %>%
  dplyr::mutate(year=(year-era)+1) %>%
  dplyr::mutate(era=(era-1960)/10) %>%
  left_join(sp_list) %>%
  dplyr::select(SPID, era, year, GID) %>%
  unique()
head(occ_grid)



```


Extraneous code Im keeping around but is not currently part of the Analysis
```{r}
# For determining species range overlaps: The Matrix Gridding Method (Unfinished)
## Intersect Range Map with grid
grid_extent <- extent(touching_cells)
xmin <- grid_extent[1]
xmax <- grid_extent[2]
ymin <- grid_extent[3]
ymax <- grid_extent[4]
cellsize <- 50*1000
xr <- (xmax - xmin)/cellsize # Calc using a xmax-xmin/cell_size?
yr <- (ymax - ymin)/cellsize
grid_int <- matrix(as.matrix(st_intersects(touching_cells, range)), nrow = xr, ncol = y)

```



      