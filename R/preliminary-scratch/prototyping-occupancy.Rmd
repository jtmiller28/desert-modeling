---
title: "prototyping-occupancy"
author: "JT Miller"
date: "2023-08-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A prototype of Occupancy Modeling: Checking to see if Vaughn Shirey's Occupancy Modeling methods can be used for the Sonoran Desert assemblages. The script: (data_process.R)[https://github.com/lmguzman/occ_historical/blob/main/analysis/odonates/scripts/data_process.R] by Vaughn was referenced to build this 

### Start out small: Model 1 common annual plant species: Rafinesquia neomexicana. 

Load Necessary Libraries 
```{r message=FALSE}
library(tidyverse); library(taxotools); library(data.table)
library(sf); library(cowplot); library(maps); library(mapdata)
library(ggpubr); library(nimble); library(raster); library(lubridate)
library(reclin); library(sp); library(rgdal); library(geosphere); library(plotKML); library(rangeBuilder)
```

Outline for Occupancy Modeling with Opportunistic Occurrence Data (as noted in Shirley et al. 2023)
      1. Determine the proportion of "Community Sampling Events" (CSEs) for the given species *Rafinesquia neomexicana*, where CSEs are defined as sampling events where more than 1 species is collected on the same day within 1km of eachother. 50% is the go-no-go cut off. 
      2. Decide on the Spatial and temporal scale of the analysis, as well as the size of the occupancy intervals & visit intervals
      3. Check the Range Overlap of the species
      4. Check taxonomic scale and Sampling methods. Should non-detections be inferred across the family and genus level? 
      5. Check whether visit history increases, decreases, or remains constant through occupancy intervals. 
      6. Remove sites that are only present for a _single_ occupancy interval. They will skew the visit history
      7. If visit history decreases through time and the probability of community sampling events is too low, re-eval from step 2 on. 
      

This prototyping is going to focus on modeling a one species as full modeling will be resource intensive, this analysis will focus on the Sonoran desert as the delimitation of Area. 

Step 1A: 
Bring in the shapefile of the Sonoran and all Plant occurrence data related to the Sonoran. 
```{r eval=FALSE, include=FALSE}
na_ecoregions_3 <- sf::read_sf("/blue/soltis/millerjared/pollen-project/Pollen-DB/raw-data/Ecoregions/na-level3-ecoregions/NA_CEC_Eco_Level3.shp") # EPA North American Ecoregion shapefiles

sonoran_shp <- subset(na_ecoregions_3, na_ecoregions_3$NA_L3NAME == "Sonoran Desert") # Subset for the shapefiles that include the Sonoran Desert

directory <- "/blue/soltis/millerjared/pollen-project/Pollen-DB/pollen-project-cluster/plant-gator-pulls-v0.4/specimen-info-datasets/" # Create a variable with the path of the dir
file_list <- list.files(directory, full.names = TRUE) # Create a list of all the file-names (which are the accepted species names in the dir)
species_tables <- list() # Create a species tables holding list
combined_table <- NULL # Initialize combined_table value as NULL 
check_line_count <- list() # Create a vector of line counts to check against parent data. 
for (i in 1:length(file_list)) { 
  df <- read.delim(file_list[[i]], fill = TRUE, quote = "") # Issue with fread dropping data, conversion necessary
  dt <- as.data.table(df) # Convert
  check_line_count[[i]] <- nrow(dt) # Check in place
  if ("Sonoran Desert" %in% dt$ecoregion) { # Require that Sonoran Desert be present for at least one of the species in the region. 
    dt[, (colnames(dt)) := lapply(.SD, as.character), .SDcols = colnames(dt)] 
    if (is.null(combined_table)) {
      combined_table <- dt
    } else {
      combined_table <- rbindlist(list(combined_table, dt), use.names = TRUE, fill = TRUE)
    }
  }
  print(paste0("table ", i, paste0(" of ", length(file_list)))) # Just a check so I can keep track of where in the loop we are
}

```

Write-out as a csv so we dont need to run this time-consuming codeblock above for future runs
```{r eval=FALSE, include=FALSE}
write.csv(combined_table, "/blue/soltis/millerjared/desert-modeling/data/sonoran-extent-plants.csv", row.names = FALSE)
```



```{r}
combined_table <- fread("/blue/soltis/millerjared/desert-modeling/data/sonoran-extent-plants.csv")
```



This is a quick check on read-continuity after noticing some odd behavior with fread. Feel free to SKIP as its just data checks. Current Status: Issues, but minimal (1-2 reads seem to be missing for 79 species) chat with Rob about SQL stuff. 
```{r eval=FALSE, include=FALSE}
# Check line count from bash-script reader to insure all lines were read 
count_read <- read.delim("/blue/soltis/millerjared/desert-modeling/outputs/line_counts.txt", header = FALSE) # see bash script for details.
run_counts <- combined_table[, .(num_rows = .N), by = accepted_name] # 
## Extract n_of_occ and accepted_name from the count_read bash file
count_read <- as.data.table(count_read)
count_read <- count_read[, n_of_occ := as.integer(gsub("^(\\d+).*", "\\1", V1)) - 1] # Also...headers count as a line so n_of_occ - 1
count_read <- count_read[, accepted_name := gsub(".*/([^/]+)\\.txt$", "\\1", V1)]
count_read <- count_read[, accepted_name := gsub("-", " ", accepted_name)]

common_names <- intersect(count_read$accepted_name, run_counts$accepted_name)

count_read <- count_read[accepted_name %in% common_names]
run_counts <- run_counts[accepted_name %in% common_names]

nrow(count_read) == nrow(run_counts) # Check to see if there is the same number of rows in each comparison table

## Check continuity 
merge_tb <- merge(count_read, run_counts, by = "accepted_name", suffixes = c("_1", "_2")) # Merge, add suffixes to avoid dup error

merge_tb <- merge_tb[, is_equivalent := n_of_occ == num_rows] # test equivalency
all_equivalent <- all(merge_tb$is_equivalent)
merge_tb[, .(.N), by = is_equivalent]
# So, in short...there are problems by 1-2 records. Unsure what is tripping this atm, this is prob a chat with Rob. For now im going with 'good enough' but will change things when we set up the SQL server. 
```

Step 1B.1: Buffer the Sonoran Shp file to account for community sampling that could occur on the region of interest's edges. Filter occurrence records to only include those found within the buffered shp.  
```{r}
crs_1 <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m"

na_ecoregions_3 <- sf::read_sf("/blue/soltis/millerjared/pollen-project/Pollen-DB/raw-data/Ecoregions/na-level3-ecoregions/NA_CEC_Eco_Level3.shp") # EPA North American Ecoregion shapefiles

sonoran_shp <- subset(na_ecoregions_3, na_ecoregions_3$NA_L3NAME == "Sonoran Desert") # Subset for the shapefiles that include the Sonoran Desert

sonoran_shp <- st_transform(sonoran_shp, crs = crs_1) # convert to albers equal area

# Optional step, if we want to add a boundary to the shapefile to account for edge cases. 
sonoran_shp_b <- st_buffer(sonoran_shp, dist = 50000) # 50,000 m = 50km 

combined_table_f <- combined_table[, latitude := as.numeric(latitude)] # Fix field class
combined_table_f <- combined_table_f[, longitude := as.numeric(longitude)]
combined_table_f <- combined_table[!is.na(latitude)] # Remove non-spatial records
combined_table_f <- combined_table_f[!is.na(longitude)]
combined_table_f <- combined_table_f[, coordinateUncertaintyInMeters := as.numeric(coordinateUncertaintyInMeters) ] # fixing the numeric

# Convert occurrence data to spatial, transfrom to the shapefiles CRS
combined_table_f <- as.data.frame(combined_table_f) # restate as a data.frame for spatial conversion
occ_spat <- st_as_sf(combined_table_f, # convert the occurrence data to a spatial object
                     coords=c("longitude", "latitude"),
                     crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs",
                     remove = FALSE)  

occ_spat <- occ_spat %>% 
  st_transform(crs = st_crs(sonoran_shp)) # Transform CRS

# Filter the data according to the shapefile boundary 
occ_spat_f <- occ_spat[sonoran_shp,] # I chose to use non-buffered shape, sub in sonoran_shp_b for buffered shp

# Now transform data back to wgs84 projection for further calculations
occ_spat_f <- st_transform(occ_spat_f, crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
```

Step 1C: Defining community sampling events by 1km distance on the same day
```{r}
occ_dat <- occ_spat_f %>% 
  dplyr::filter(coordinateUncertaintyInMeters < 100*1000 | is.na(coordinateUncertaintyInMeters)) # remove data based upon uncertainty 

versioned_dat <- occ_dat

comm_collxs <- versioned_dat %>% # take from versioned dat...
  dplyr::select(accepted_name, eventDate, recordedBy, latitude, longitude) %>%  # select cols of interest
  dplyr::distinct() %>% # run distinct on cols of interest
  dplyr::filter(!is.na(eventDate), !is.na(latitude), !is.na(longitude)) %>%  # remove NAs for location and date
  dplyr::filter(!is.na(recordedBy)) %>% # remove NAs for no recordedBy
  mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>%  # fix time stamped dates
  mutate(date_clean = ymd(fixedEventDate)) %>% # fix up date into ymd using lubridate
  filter(date_clean > ymd("1950-01-01")) %>% # set a min date
  filter(date_clean >= ymd("2020-12-31")) # set a max date
  
comm_collxs$recordedBy %>% table() %>% # take the comms_collxs and create a table
  sort(decreasing = TRUE) %>% # Sort by greatest to least
  head() # take the first 5 

# Look at community sampling by same day within 1km of eachother...
comm_collxs <- versioned_dat %>%  # take from versioned dat...
  dplyr::select(accepted_name, eventDate, recordedBy, latitude, longitude) %>% 
  distinct() %>% 
  dplyr::filter(!is.na(eventDate), !is.na(latitude), !is.na(longitude)) %>% 
  dplyr::filter(!is.na(recordedBy)) %>% 
  mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>%  # fix time stamped dates
  mutate(date_clean = ymd(fixedEventDate)) %>% # fix up date into ymd using lubridate
  filter(date_clean > ymd("1950-01-01")) %>% # set a min date
  filter(date_clean <= ymd("2020-12-31")) %>% 
  data.table()

n_obs_day <- table(comm_collxs$date_clean) # create a freq table of the dates in our comm_collxs
single_obs_dates <- names(n_obs_day[n_obs_day == 1]) # where only single observations were made
unique_dates <- names(n_obs_day[n_obs_day > 1]) # where multiple observations were made

cluster_lists <- list() # create a holder for the cluster_lists
sin_obs_data <- comm_collxs[date_clean %in% ymd(single_obs_dates)] # create a dt that contains the data that corresponds with the single obs dates
sin_obs_data[, "cluster" := paste0(date_clean, "-", 1)] # create a new field called cluster, paste the clean date from the single obs dates + "-1" to denote that these are single obs events
cluster_lists[[1]] <- sin_obs_data # embed these as the first element in cluster lists
counter <- 2 # establish a counter var

for(date_use in unique_dates){ # recall that date_use is i here... pulling from our unique_dates (Mult observs)
  cur_date <- comm_collxs[date_clean == date_use] # subset comm_collxs by unique mult dates
  
  lat_lon <- cur_date[, .(longitude, latitude)] # subset for only lat lon
  
  xy <- SpatialPointsDataFrame( # create a spatial points dataframe 
    lat_lon, # specified in lat lon
    data.frame(ID = seq(1:nrow(lat_lon))),  # create a field called ID based upon each lat lon
    proj4string = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84") # specify our crs as wgs84
    )
  
  # use distm to generate a geodesic distance matrix in meters
  mdist <- distm(xy)
  
  # cluster all points using a hierarchical clustering approach
  hc <- hclust(as.dist(mdist), method = "complete")
  
  # define the distance threshold to use
  d = 1000 # 1km
  
  cur_date[, "cluster" := paste0(date_clean, "-", cutree(hc, h=d))] # create the cluster field based upon the number of clustered points
  
  cluster_lists[[counter]] <- cur_date # storage for cluster_lists based upon where we are 
  
  counter <- counter + 1 # add one per iteration
  
  print(paste0("subset ", date_use, paste0(" of ", max(unique_dates)))) # Print out dates to note where we are 
}

all_clusters <- rbindlist(cluster_lists) # rbind our cluster lists into a df

size_clusters <- table(all_clusters$cluster) %>% # create a freq table
  table() # duplicates(?)

size_clusters[1]/sum(size_clusters[1:length(size_clusters)]) # measure the proportion of community samplings to total samplings

```
1C (Same as above, but with the field recordedBy removed)
```{r}
occ_dat <- occ_spat_f %>% 
  dplyr::filter(coordinateUncertaintyInMeters < 100*1000 | is.na(coordinateUncertaintyInMeters)) %>%  # remove data based upon uncertainty 
  st_drop_geometry()

versioned_dat <- occ_dat

comm_collxs <- versioned_dat %>% # take from versioned dat...
  dplyr::select(accepted_name, eventDate, recordedBy, latitude, longitude) %>%  # select cols of interest
  dplyr::distinct() %>% # run distinct on cols of interest
  dplyr::filter(!is.na(eventDate), !is.na(latitude), !is.na(longitude)) %>%  # remove NAs for location and date
  dplyr::filter(!is.na(recordedBy)) %>% # remove NAs for no recordedBy
  mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>%  # fix time stamped dates
  mutate(date_clean = ymd(fixedEventDate)) %>% # fix up date into ymd using lubridate
  filter(date_clean > ymd("1970-01-01")) %>% # set a min date
  filter(date_clean >= ymd("2020-12-31")) # set a max date
  
comm_collxs$recordedBy %>% table() %>% # take the comms_collxs and create a table
  sort(decreasing = TRUE) %>% # Sort by greatest to least
  head() # take the first 5 

# Look at community sampling by same day within 1km of eachother...
comm_collxs <- versioned_dat %>%  # take from versioned dat...
  dplyr::select(accepted_name, eventDate, latitude, longitude) %>% 
  distinct() %>% 
  dplyr::filter(!is.na(eventDate), !is.na(latitude), !is.na(longitude)) %>% 
  mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>%  # fix time stamped dates
  mutate(date_clean = ymd(fixedEventDate)) %>% # fix up date into ymd using lubridate
  filter(date_clean > ymd("1950-01-01")) %>% # set a min date
  filter(date_clean <= ymd("2020-12-31")) %>% 
  data.table()

n_obs_day <- table(comm_collxs$date_clean) # create a freq table of the dates in our comm_collxs
single_obs_dates <- names(n_obs_day[n_obs_day == 1]) # where only single observations were made
unique_dates <- names(n_obs_day[n_obs_day > 1]) # where multiple observations were made

cluster_lists <- list() # create a holder for the cluster_lists
sin_obs_data <- comm_collxs[date_clean %in% ymd(single_obs_dates)] # create a dt that contains the data that corresponds with the single obs dates
sin_obs_data[, "cluster" := paste0(date_clean, "-", 1)] # create a new field called cluster, paste the clean date from the single obs dates + "-1" to denote that these are single obs events
cluster_lists[[1]] <- sin_obs_data # embed these as the first element in cluster lists
counter <- 2 # establish a counter var

for(date_use in unique_dates){ # recall that date_use is i here... pulling from our unique_dates (Mult observs)
  cur_date <- comm_collxs[date_clean == date_use] # subset comm_collxs by unique mult dates
  
  lat_lon <- cur_date[, .(longitude, latitude)] # subset for only lat lon
  
  xy <- SpatialPointsDataFrame( # create a spatial points dataframe 
    lat_lon, # specified in lat lon
    data.frame(ID = seq(1:nrow(lat_lon))),  # create a field called ID based upon each lat lon
    proj4string = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84") # specify our crs as wgs84
    )
  
  # use distm to generate a geodesic distance matrix in meters
  mdist <- distm(xy)
  
  # cluster all points using a hierarchical clustering approach
  hc <- hclust(as.dist(mdist), method = "complete")
  
  # define the distance threshold to use
  d = 1000 # 1km
  
  cur_date[, "cluster" := paste0(date_clean, "-", cutree(hc, h=d))] # create the cluster field based upon the number of clustered points
  
  cluster_lists[[counter]] <- cur_date # storage for cluster_lists based upon where we are 
  
  counter <- counter + 1 # add one per iteration
  
  print(paste0("subset ", date_use, paste0(" of ", max(unique_dates)))) # Print out dates to note where we are 
}

all_clusters <- rbindlist(cluster_lists) # rbind our cluster lists into a df

size_clusters <- table(all_clusters$cluster) %>% # create a freq table
  table() # duplicates(?)

size_clusters[1]/sum(size_clusters[1:length(size_clusters)]) # measure the proportion of community samplings to total samplings

print(size_clusters[1]/sum(size_clusters[1:length(size_clusters)])) # 48.6% on last run. 
```


Make Grids + Ranges for indexing
```{r}
sp_list <- as.data.frame(table(occ_dat$accepted_name)) #%>% 
  #dplyr::filter(Freq >= 100) # create a species list where there is at least 100 specimens present in the data

occ_spatial <- occ_dat %>% # create a spatial dataset
  dplyr::select(accepted_name, eventDate, recordedBy, latitude, longitude) %>% 
  filter(!is.na(accepted_name), accepted_name != "", accepted_name %in% sp_list$Var1) %>% 
  mutate(fixedEventDate =  gsub("T\\d+:\\d+:\\d+", "", eventDate)) %>% # fix timestamped dates
  mutate(date_clean = ymd(fixedEventDate)) %>% # set to ymd using lubridate
  filter(date_clean > ymd("1950-01-01")) %>% # set a min date
  filter(date_clean <= ymd("2020-12-31")) %>% # set a max date
  st_as_sf(coords = c("longitude", "latitude"), 
           crs = "+proj=longlat +ellps=WGS84 +datum=WGS84") %>%  # set as spatial 
  st_transform(crs_1) # transform into albers
 
grid_1 <- st_make_grid(extent(sonoran_shp), cellsize = 50*1000, square = FALSE) %>% # 50x50km hex grids
  st_as_sf(crs = crs_1) %>% # Assign CRS
  dplyr::mutate(GID = row_number()) # Track Grid IDs by row number in our datatable

touching_cells <- grid_1[sonoran_shp,] # define the cells that touch the shapefile 

ggplot() + # show this 
  geom_sf(sonoran_shp, mapping = aes(), fill = "darkorange") +
  geom_sf(touching_cells, mapping = aes(), alpha = 0.2) +
  theme_bw() +
  ggtitle("Hexagon Intersection Method of Creating a grid over the Sonoran")


# We'll now format occurrence data for Vaughn's data.prep fxn.
# Two cases: all & detection
# All: all species at all sites
# Detection: species where (at least) on other species was detected
# Range maps inferred by convex_hulls derived from occurrences 

### make range maps from convex polygons
range <- list()
sp_list <- unique(occ_spatial$accepted_name)
for(i in 1:length(sp_list)){
  range[[i]] <- st_convex_hull(st_union(filter(occ_f,
                                               accepted_name==sp_list[i])))
}

test <- getDynamicAlphaHull(st_union(filter(occ_spatial, accepted_name[[]])))

```






1.D Tidy up the table for spatial/temporal scale analysis on the relevant species 
```{r}
occ_dat <- occ_f %>% 
  filter(!is.na(accepted_name), accepted_name!= "", accepted_name %in% sp_list$accepted_name) %>% # Filter records based on valid name + 100 record quota
  filter(coordinateUncertaintyInMeters < 100*1000 | is.na(coordinateUncertaintyInMeters)) %>%  # Filter records based on uncertainty
  filter(is.na(eventDate)) %>%  # Remove records with NA date
  mutate(eventDate = gsub("T00:00:00", "", eventDate)) %>%  # fix timestamped dates
  mutate(clean_date = ymd(eventDate)) %>%  # Standardize dates
  filter(!is.na(clean_date)) %>%  # Remove NA cleaned dates 
  filter(clean_date > ymd("1950-01-01") & clean_date < ymd("2020-12-31")) # Only take data for this timeframe
  

  
  
```




2. Decide on the Spatial and temporal scale of the analysis, as well as the size of the occupancy intervals & visit intervals
```{r}
# First lets take a look at the spatial representation
## set the project crs to North American Equal Area Conic (Albers)
crs_1 <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m"
## Change our buffered shapefile to match this equal area proj
sonoran_shp_b <- st_transform(sonoran_shp_b, st_crs(crs_1))
## Make a Grid with 50km gridcells
grid_1 <-st_make_grid(extent(sonoran_shp_b), cellsize = 50*1000, square = FALSE) %>% # Make a hexagonal grid from the extents of our buffered region
          st_as_sf(crs = crs_1) %>% # Assign CRS
          dplyr::mutate(GID = row_number()) # Track Grid IDs by row number in our datatable


touching_cells <- grid_1[sonoran_shp_b,]

ggplot() + 
  geom_sf(sonoran_shp_b, mapping = aes(), fill = "yellow")+
  geom_sf(sonoran_shp, mapping = aes(), fill = "darkorange") +
  geom_sf(touching_cells, mapping = aes(), alpha = 0.2) +
  theme_bw() +
  ggtitle("Hexagon Intersection Method of Creating a grid over the Sonoran")
  

## Make convex-hull to show known range
occ_f <- st_as_sf(occ_f, 
           coords=c("longitude", "latitude"),
           crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs") 

occ_f <- occ_f %>% st_transform(crs_1) # transform to equal area proj

# make range maps from convex polygons
range <- list()
sp_list <- unique(occ_f$accepted_name)
for(i in 1:length(sp_list)){
  range[[i]] <- st_convex_hull(st_union(filter(occ_f,
                                               accepted_name==sp_list[i])))
}

ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(grid_1, mapping=aes(), fill=NA, color="grey")+
  geom_sf(range[[1]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[1]), mapping=aes(), color="cyan")+
  geom_sf(range[[2]], mapping=aes(), fill="red", color="red", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[2]), mapping=aes(), color="red")+
  geom_sf(range[[10]], mapping=aes(), fill="gold", color="gold", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[10]), mapping=aes(), color="gold")+
  theme_map()

ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(grid_1, mapping=aes(), fill=NA, color="grey")+
  geom_sf(range[[3872]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[3872]), mapping=aes(), color="royalblue")+
  ggtitle(paste0("Species Range for ", sp_list[3872])) +
  theme_map()

# Vaughn's code uses a matrix to assess range overlap, but this a bit challenging with irregularly shaped grids. By their suggestion lets try vectorizing: Using the hex's unique GIDs we're going to index the overlap in ranges. 

## Proof of concept: 
grid_copy <- touching_cells  # Make a copy of the hex cells that matter
test <- st_intersects(touching_cells, range[[3872]]) # Find all of the intersecting hex cells that touch the range of R. neomexicana

grid_copy$intersection <- as.integer(test) # Returns 1s for the TRUE values, for some reason returns NAs instead of 0s for FALSE? 

grid_copy <- grid_copy %>% 
  mutate(intersection = case_when( # Use a case_when to ammend this
    !is.na(intersection) ~ 1, 
    TRUE ~ 0
  ))

# Show a visual... 
ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(touching_cells, mapping = aes(), color = "grey", alpha = 0.2) +
  geom_sf(grid_copy[grid_copy$intersection == 1,], mapping=aes(), fill=NA, color="red")+
  geom_sf(range[[3872]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[3872]), mapping=aes(), color="royalblue")+
  ggtitle(paste0("Species Range for ", sp_list[3872])) +
  theme_map()
```
Now Create a loop for indexing all of the present cells per plant range. 
```{r}
grid_int <- list()
for(i in 1:length(range)){
  grid_copy <- touching_cells # Create a fresh copy per run
  hex_int <- st_intersects(touching_cells, range[[i]]) # Return a logical output for whether the species range intersects with the hex cells
  grid_copy$intersection <- as.integer(hex_int) # Returns 1s for the TRUE values, for some reason returns NAs instead of 0s for FALSE (?) 
  grid_copy <- grid_copy %>% 
  mutate(intersection = case_when( # Use a case_when to ammend this
    !is.na(intersection) ~ 1, 
    TRUE ~ 0))
  
  grid_int[[i]] <- grid_copy # And store these intersection grids
  print(paste0("species ", i, paste0(" of ", length(range))))
}

# Check...
ggplot()+
  geom_sf(data=sonoran_shp, mapping=aes())+
  geom_sf(touching_cells, mapping = aes(), color = "grey", alpha = 0.2) +
  geom_sf(grid_int[[3872]][grid_int[[3872]]$intersection == 1,], mapping=aes(), fill=NA, color="red")+
  geom_sf(range[[3872]], mapping=aes(), fill="cyan", color="cyan", alpha=0.35)+
  geom_sf(filter(occ_f, accepted_name==sp_list[3872]), mapping=aes(), color="royalblue")+
  ggtitle(paste0("Species Range for ", sp_list[3872])) +
  theme_map()

## assign occurrences to the indexes 
sp_list <- sp_list %>% as.data.frame()
colnames(sp_list) <- c("accepted_name")

sp_list <- sp_list %>%
  dplyr::mutate(SPID=row_number())


occ_f$year <- as.integer(occ_f$year)

occ_grid <- occ_f %>%
  st_intersection(touching_cells) %>% # Intersect with the toching grids
  dplyr::mutate(era=(year-year%%10)) %>%
  dplyr::mutate(year=(year-era)+1) %>%
  dplyr::mutate(era=(era-1960)/10) %>%
  left_join(sp_list) %>%
  dplyr::select(SPID, era, year, GID) %>%
  unique()
head(occ_grid)



```


Extraneous code Im keeping around but is not currently part of the Analysis
```{r}
# For determining species range overlaps: The Matrix Gridding Method (Unfinished)
## Intersect Range Map with grid
grid_extent <- extent(touching_cells)
xmin <- grid_extent[1]
xmax <- grid_extent[2]
ymin <- grid_extent[3]
ymax <- grid_extent[4]
cellsize <- 50*1000
xr <- (xmax - xmin)/cellsize # Calc using a xmax-xmin/cell_size?
yr <- (ymax - ymin)/cellsize
grid_int <- matrix(as.matrix(st_intersects(touching_cells, range)), nrow = xr, ncol = y)

```



      